{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CS231n Notes","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This repository will hold notes on CS231n course.</p>"},{"location":"#project-layout","title":"Project Layout","text":"<pre><code>.\n\u251c\u2500 assignments/\n\u2502  \u2514\u2500 assignment1\n\u2502  \u2514\u2500 assignment2\n\u2502  \u2514\u2500 assignment3\n\u251c\u2500 docs/\n\u2502  \u2514\u2500 content/\n\u2502  \u2514\u2500 index.md\n\u2514\u2500 mkdocs.yml\n</code></pre>"},{"location":"content/Architecture/","title":"CNN Architectures","text":""},{"location":"content/Architecture/#quick-recap-on-cnn","title":"Quick Recap on CNN","text":"<p>A typical CNN architecture contains three types of layers, respectively being convolution layer, pooling layer (including max-pooling and average pooling), fully connected layer.</p> <p>Convolution Layer</p> <p>The convolution layer comes with layers of filters, which are essentially a stack of smaller 3D matrices with the same depth as the input serving as the weight as in the fully connected layer.</p> \\[ R = \\dfrac{W + 2 \\times P - F}{S} + 1 \\quad \\quad \\quad \\quad \\text{[1]} \\] <p>The equation \\([1]\\) describes the output matrix size after a convolution in which \\(P\\) stands for padding, \\(F\\) stands for filter size (kernel size) and \\(S\\) stands for stride.</p> <p>Pooling Layer</p> <p>The pooling layer is used to reduce the matrix size. Just like what we did in convolution process, we apply a filter to the matrix except that this filter does not have any parameters.</p> <p>The filter in max pooling operation servers to extract the maximum value within the cells covered by the filter as it moves \\(S\\) (stride) number of steps ahead.</p> <p>Rather than performing the max operation, the general pooling performs the average operation as it moves \\(S\\) (stride) number of steps ahead.</p> \\[ R = \\dfrac{W - F}{S} + 1 \\quad \\quad \\quad \\quad \\text{[2]} \\] <p>Equation \\([2]\\) describes the general method for calculating the matrix size after a pooling operation.</p> <p>Fully Connected Layer</p> <p>As the name suggests, layers in the rear end of CNN are fully connected layers as in regular neural network. The operation here is the same as that in regular neral network.</p>"},{"location":"content/Architecture/#batch-normalization","title":"Batch Normalization","text":""},{"location":"content/Architecture/#definition","title":"Definition","text":"<p>Sometimes after a dot product, the values in the matrix may become either too large or too small making it hard for the network to train (values with 0 mean and 1 std is considered suitable for network training). To address this problem, we introduce the batch normalization.</p> \\[ \\begin{align} \\mu &amp;= \\dfrac{1}{N} \\sum_{k=1}^{N} x_k \\\\ \\sigma^2 &amp;= \\dfrac{1}{N} \\sum_{k=1}^{N} (x_k - \\mu)^2 \\\\ \\hat{x} &amp;= \\dfrac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\ \\hat{y} &amp;= \\gamma \\times \\hat{x} + \\beta \\end{align} \\] <p>For a \\(N \\times D\\) input matrix, batch normalization first sums over axis \\(0\\) to compute \\(\\mu\\) and \\(\\sigma\\), resulting in two \\(1 \\times D\\) matrices. With these two matrices, we can then calculate the \\(\\hat{x}\\) by performing normalization. Finally, use \\(\\gamma\\) (scale) and \\(\\beta\\) (shift) values to add learnable parameters.</p> <p>One thing to notice is that if \\(\\gamma = \\sigma\\) and \\(\\beta = \\mu\\), the normalization process will bring the input back to its original state.</p> <p>Another thing to notice is that batch normalization is only valid at training time since in each training epoch, a batch of input will be sent in while at test time, only one sample is passing into the model making batch normalization meaningless.</p> <p>To mitigate this problem, we can use running mean and running variance method. Basically, during training time, we constantly accumulate the mean and variance values we calculated with the help of a new parameter -- \\(\\rho\\).</p> \\[ \\begin{align} running\\_meaning &amp;= \\rho \\times running\\_meaning + (1 - \\rho) + mean \\\\ running\\_variance &amp;= \\rho \\times running\\_variance + (1 - \\rho) + variance \\end{align} \\]"},{"location":"content/Architecture/#forward","title":"Forward","text":"<p>The forward of batch normalization involves only the use of <code>np.mean</code> and <code>np.var</code> with the <code>axis</code> parameter set to <code>0</code> and <code>keepdims</code> set to <code>True</code>.</p> <p>For detailed code implementation, see <code>assignment2/cs231n/layers.py</code>.</p>"},{"location":"content/Architecture/#backward","title":"Backward","text":"<p>Usually when we try to back propagate a given function, a computation graph will be of great help. The key to draw a correct computation graph lies in determining the exact number of inputs and the interconnection between each calculated parts.</p> \\[ \\begin{align} \\mu &amp;= \\dfrac{1}{N} \\sum_{k=1}^{N} x_k \\\\ \\sigma^2 &amp;= \\dfrac{1}{N} \\sum_{k=1}^{N} (x_k - \\mu)^2 \\\\ \\hat{x} &amp;= \\dfrac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\ \\hat{y} &amp;= \\gamma \\times \\hat{x} + \\beta \\end{align} \\] <p>The computation graph for the above equations will be the following.</p> Fig1. Batch Norm Forward Graph <p>The first key takeaway here is \\(x - \\mu\\). Why should we add another \\(x\\) pointing to the \\(\\mu\\) given that we can derive this equation all at once? The reason lies in the fact that we're reusing \\(x\\) in this case, and it means we're introducing an external \\(x\\) value which should be back propagated to in the backward pass.</p> <p>Another key takeaway is that there's a connection between \\(\\mu\\) and \\(\\sigma^2\\). The same holds true in this case. According to the equation for calculating the \\(\\sigma^2\\), we reuse the \\(\\mu\\) term and it should be back propagated to as well.</p> <p>The backward computation graph is displayed below.</p> Fig1. Batch Norm Backward Graph <p>The sample code for back prop is shown below (The variable <code>cache</code> originates from the forward method. Refer to <code>layers.py</code> for in-detail implementation).</p> <pre><code>def batchnorm_backward(dout, cache):\n    \"\"\"Backward pass for batch normalization.\n\n    For this implementation, you should write out a computation graph for\n    batch normalization on paper and propagate gradients backward through\n    intermediate nodes.\n\n    Inputs:\n    - dout: Upstream derivatives, of shape (N, D)\n    - cache: Variable of intermediates from batchnorm_forward.\n\n    Returns a tuple of:\n    - dx: Gradient with respect to inputs x, of shape (N, D)\n    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n    \"\"\"\n    dx, dgamma, dbeta = None, None, None\n    ###########################################################################\n    # TODO: Implement the backward pass for batch normalization. Store the    #\n    # results in the dx, dgamma, and dbeta variables.                         #\n    # Referencing the original paper (https://arxiv.org/abs/1502.03167)       #\n    # might prove to be helpful.                                              #\n    ###########################################################################\n    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n\n    x_hat = cache[\"x_hat\"]\n    x_origin = cache[\"x_origin\"]\n    gamma = cache[\"gamma\"]\n    var = cache[\"var\"]\n    avg = cache[\"avg\"]\n\n    N, _ = dout.shape\n    dbeta = np.sum(dout, axis=0)\n    dgamma = np.sum(x_hat * dout, axis=0)\n\n    # dvalues from previous layers\n    dx_hat = dout * gamma\n    dvar = np.sum(dx_hat * (x_origin - avg) * -0.5 / np.sqrt(var ** 3), axis=0)\n    dmu = np.sum(dx_hat * -1 / np.sqrt(var), axis=0) + dvar * np.sum(-2.0 * (x_origin - avg), axis=0) / N\n\n    # Recall in the forward graph, there're three x contributing to the results\n    dx = dx_hat / np.sqrt(var) +  dvar * 2 * (x_origin - avg) / N + dmu / N\n\n    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n    ###########################################################################\n    #                             END OF YOUR CODE                            #\n    ###########################################################################\n\n    return dx, dgamma, dbeta\n</code></pre>"},{"location":"content/Architecture/#layer-normalization","title":"Layer Normalization","text":"<p>Other than batch normalization, layer normalization is another common technique. Instead of summing over the \\(0\\) axis, layer normalization sums over axis \\(1\\).</p> <p>To show the difference in detail, consider the following code.</p> <pre><code># Batch Norm\navg = np.mean(x, axis=0, keepdims=True)\nvar = np.var(x, axis=0, keepdims=True)\n\n# Layer Norm\navg = np.mean(x, axis=1, keepdims=True)\nvar = np.var(x, axis=1, keepdims=True)\n</code></pre> <p>The most important difference between layer norm and batch norm is that layer norm can be safely run in test time without the running mean or running var technique.</p>"},{"location":"content/Architecture/#forward_1","title":"Forward","text":"<p>The forward method is partially shown in the above code sample. For detailed implementation, see <code>layers.py</code> file.</p>"},{"location":"content/Architecture/#backward_1","title":"Backward","text":"<p>The backward of layer norm runs nearly the same as the batch norm except that batch norm sums over the number of samples while layer norm sums over the number of features which leads to a different choice of axis and a different choice of division number.</p> <p>To illustrate, consider the core part of backward pass in batch norm.</p> <pre><code>dx_hat = dout * gamma\ndvar = np.sum(dx_hat * (x_origin - avg) * -0.5 / np.sqrt(var ** 3), axis=0)\ndmu = np.sum(dx_hat * -1 / np.sqrt(var), axis=0) + dvar * np.sum(-2.0 * (x_origin - avg), axis=0) / N\n</code></pre> <p>We sum the <code>dvar</code> based on the \\(0\\) axis and we divide part of <code>dmu</code> by <code>N</code>.</p> <p>In layer norm, however, we must sum over axis \\(1\\) and keep the dimension along the way because that's how we calculate the normalization in the first place. Also, rather than dividing by <code>N</code>, we should now divide by <code>D</code> (the second dimension of <code>dout</code>).</p>"},{"location":"content/Convolution/","title":"Convolution Neural Network","text":""},{"location":"content/Convolution/#basic-architecture","title":"Basic Architecture","text":"<p>I'd suggest to read through the official course notes since it provides really detailed information and intuitive description.</p>"},{"location":"content/Convolution/#tricks-notes","title":"Tricks &amp; Notes","text":"<p>FC -&gt; Conv Layer</p> <p>A fully connected layer can be converted into a convolution layer. For example, consider an output after the final pooling (convolution) process with the size of [7, 7, 512]. A typical fc layer would flatten the entire matrix and perform dot product with the next fc layer (typically with size of 4096).</p> <p>From another point of view, we can apply a 4096, [7, 7] filter with stride 1 on this output, which yields [1, 1, 4096] output shape. With another 1000, [1, 1] filter, we can calculate the class scores and give out an output of shape [1, 1, 1000]. All that we need to do is reshaping it back to 2D.</p>"},{"location":"content/Optimization/","title":"Optimization","text":""},{"location":"content/Optimization/#computing-the-gradients","title":"Computing the Gradients","text":"<p>There're two ways to compute the gradients: numerical and analytical.</p>"},{"location":"content/Optimization/#numerical","title":"Numerical","text":"<p>Recall when we first get our hands on calculating the derivative of a function, we define the following method:</p> \\[ f'(x) = \\lim_{h \\to 0} \\dfrac{f(x+h) - f(x)}{h} \\] <p>\\(h\\) is typically a small enough value since strictly speaking, the mathematical definition requires \\(h\\) to be close enough to \\(0\\).</p> <p>Moving it to broader dimensions, we can calculate the partial derivative by iterating through each dimension using the above method.</p> <p>Example code:</p> <pre><code>def eval_numerical_gradient(f, x):\n  \"\"\"\n  a naive implementation of numerical gradient of f at x\n  - f should be a function that takes a single argument\n  - x is the point (numpy array) to evaluate the gradient at\n  \"\"\"\n\n  fx = f(x) # evaluate function value at original point\n  grad = np.zeros(x.shape)\n  h = 0.00001\n\n  # iterate over all indexes in x\n  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n  while not it.finished:\n\n    # evaluate function at x+h\n    ix = it.multi_index\n    old_value = x[ix]\n    x[ix] = old_value + h # increment by h\n    fxh = f(x) # evalute f(x + h)\n    x[ix] = old_value # restore to previous value (very important!)\n\n    # compute the partial derivative\n    grad[ix] = (fxh - fx) / h # the slope\n    it.iternext() # step to next dimension\n\n  return grad\n</code></pre>"},{"location":"content/Optimization/#analytical","title":"Analytical","text":"<p>The analytical method basically precalculates a formula for the actual gradients and implement that function with vectorized code.</p> <p>It's important, however, to apply a gradient check on the analytical results to make sure our calculation is correct. This so-called check simply compare the analytical results with the numerical results for equality.</p>"},{"location":"content/RNN/","title":"RNN","text":"<p>So far, we've seen and practiced the so-called \"Feed Forward\" neural network which takes in inputs and outputs either a class score or regression value.</p> <p>But what if the input is a sequence and our required output is another sequence? For example, in terms of machine translation, our target is to translate a sequence of characters into another language.</p> <p>RNN (Recurrent Neural Network) is born to do that. The vanilla RNN is pretty by definition. The basic building block receives an input, performs some operation with another variable called \"hidden state\" in the block and outputs the vector. This new vector serves as the hidden state of the next building block.</p> <p>With this basic structure, a bunch of variances can be constructed. The many to one structure takes in a sequence of inputs and outputs one vector. The one to many structure takes in one vector and outputs a sequence vectors. The similar thing applies to many to many structure.</p> <p>Of the 3 variances mentioned, the \"many-to-one\" and \"one-to-many\" are also referred to as encoder and decoder respectively.</p> <p>Tip</p> <p>I strongly recommend viewing the original slides and the course given by Michigan online</p>"},{"location":"content/RNN/#example","title":"Example","text":"<p>Suppose we're trying to build a model that takes a characters and predict the next character iteratively (something similar to GPT).</p>"},{"location":"content/RNN/#calculating-hidden-state","title":"Calculating Hidden State","text":"<p>The first we want to do is finding a suitable way to represent each letter. In this case we'll just use an one-hot vector to encode each character. The next thing is calculating the hidden state for the next building block.</p> <p>To be more specific, we have the following formula.</p> \\[ h_t = f_W(h_{t-1}, x) \\] <p>\\(f_W\\) is a short hand for weight matrix. To expand the above formula, we have:</p> \\[ h_t = f\\left(W \\cdot \\begin{pmatrix} h_{t-1} \\\\ x \\end{pmatrix}\\right) \\] <p>Notice here we're performing a dot product. It's correct but considering the fact that \\(x\\) is a one-hot vector, performing a dot product is slower comparing to just selecting the corresponding weight vector, which leads us to Embedding Layer. Embedding layer is nothing more than the dot product between a one-hot vector and a weight matrix.</p> <p>Now that we've finished calculating the next hidden state and output a character, the logical step is to input this predicted character to the same model and keep on with the exact training process.</p>"},{"location":"content/RNN/#loss-backprop","title":"Loss, BackProp","text":"<p>As a reasonable next move, we should now think about the loss function and the back propagation procedure.</p> <p>Without diving into computation details, these two processes seem pretty normal. However, what if our model has a depth of 100 or even 1000 since we might want our model to translate the entire paragraph or even the entire article? In such cases, the back propagation process will need to store a ton of weight, input, hidden state matrices just for a single backward pass. Our GPU memory will easily run out.</p> <p>That's why Truncated Back Propagation is put forward. The basic idea for truncated backward is that the entire network is truncated into parts of chunks. For each chunk of regular size, we perform the forward pass operation, store the hidden state matrix and compute the backward gradients. For the next chunk, all we need is the newly-computed hidden state, without the need for previous matrices since they're already updated.</p>"},{"location":"content/RNN/#another-example","title":"Another Example","text":"<p>Another application for RNN is image captioning. This can be done in 3 steps.</p> <p>Extracting Image Features</p> <p>The raw image first passes through a convolution layer to extract the image features.</p> <p>Add Features to RNN</p> <p>Recall the formula above where we calculate the current hidden state.</p> \\[ h_t = f_W(h_{t-1}, x) \\] <p>We simply add another parameter to this function.</p> \\[ h_t = f_W(h_{t-1}, x, img) \\] <p>But what is the \\(x\\)?</p> <p>Defining Start And End Token</p> <p>In the case of image captioning, there's no initial character to inform the model to starting predicting the next. To resolve this, we need to set a special token &lt;start&gt; to enable the prediction sequence. Similarly, the model should output a &lt;end&gt; token indicating the end of prediction.</p>"},{"location":"content/RNN/#problems","title":"Problems","text":"<p>The problem with this architecture of RNN is that the gradients are prone to becoming stagnant.</p> <p>Note</p> <p>By saying \"this architecture\", I actually mean the \\(f\\) function is \\(\\tanh\\) function</p> <p>This is because as we back propagates, we're actually accumulating the multiplication result of weight matrix. The weight matrix may either be greater than 1 or less than 1. But if we cumulatively multiply this small scalar value (for example, consider \\(1.2^{100}\\)), the result will either be so big that exceeds the maximum a computer can represent or small enough that can be ignored. The gradients would then fall into stagnancy, often called vanishing gradients.</p>"},{"location":"content/RNN/#lstm","title":"LSTM","text":"<p>To keep the gradients flowing, LSTM introduces another state called cell state.</p> Fig1. LSTM Forward Fig2. LSTM Backward <p>In Fig2, we can see the cell state flows backward without obstacle since it only involves a sum gate and a multiplication gate, the former giving a gradient of 1 and the latter giving the value of \\(f\\). Since \\(f\\) is sigmoid function, the value is always within range \\((0, 1)\\)</p> <p>One thing to keep in mind is that LSTM does not eradicate the vanishing/exploding gradients problem. They still exists in computing the gradients for \\(h\\) (hidden state). The LSTM simply adds a flowing path for cell state to update so that the entire network would not run into stagnant.</p>"},{"location":"content/SVM/","title":"SVM","text":""},{"location":"content/SVM/#forward","title":"Forward","text":"<p>The loss of each class is calculated as below:</p> \\[ \\begin{equation} L = \\sum_{j}^{j \\neq y_i} \\max(0, S_j - S_{y_i} + \\delta) \\end{equation} \\] <p>An example will better illustrate the process.</p> <p>Suppose there are 3 classes and each score is \\(s_1\\), \\(s_2\\), \\(s_3\\) and this image belongs to label \\(1\\), i.e. \\(y_i = 1\\). Also, \\(\\delta\\) is set to 1 as default.</p> \\[ L = max(s_2 - s_1 + 1, 0) + max(s_3 - s_1 + 1, 0) \\] <p>Notice that we didn't take \\(max(s_1 - s_1 + 1, 0)\\) since \\(s_1\\) is our target label.</p>"},{"location":"content/SVM/#svm-nature","title":"SVM Nature","text":"<p>SVM essentially wants the loss of correct class to be larger than the incorrect classes by at least \\(\\delta\\). If this is not the case, we will accumulate loss.</p>"},{"location":"content/SVM/#backward","title":"Backward","text":"<p>When it comes to calculating the gradients, we need to write down every single equation and with the help of chain rule, we connect each separate part together.</p> \\[ \\begin{align} L &amp;= \\dfrac{1}{N} \\sum_{i=1}^{N} L_i \\\\   &amp;= \\dfrac{1}{N} \\sum_{i=1}^{N} (\\sum_{j}^{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1)) \\end{align} \\] <p>To calculate the gradients:</p> \\[ \\begin{align} \\dfrac{\\partial L}{\\partial W} &amp;= \\dfrac{1}{N} \\sum_{i=1}^{N} \\dfrac{\\partial L_i}{\\partial W} \\\\ \\end{align} \\] <p>Abstract it:</p> \\[ \\begin{align} \\dfrac{\\partial L_i}{\\partial W} &amp;= \\sum_{k=1}^{C} \\dfrac{\\partial L_i}{\\partial s_k} \\times \\dfrac{\\partial s_k}{\\partial W} \\end{align} \\] <p>Furthermore:</p> \\[ \\begin{align} \\dfrac{\\partial L_i}{\\partial s_k} &amp;= \\dfrac{\\partial \\sum_{j}^{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1)}{\\partial s_k} \\\\                                    &amp;= \\sum_{j}^{j \\neq y_i} \\dfrac{\\partial \\max(0, s_j - s_{y_i} + 1)}{\\partial s_k} \\\\                                    &amp;= \\begin{cases}                                       0 &amp; \\text{if } j = y_i \\text{ or } k \\neq j \\text{ or } s_j - s_{y_i} + 1 \\leq 0 \\\\                                       1 &amp; \\text{otherwise }                                       \\end{cases} \\end{align} \\] <p>in which \\(s_k\\) is short for \\(s_k - s_{y_i} + 1\\). This may seem a little unintuitive. Just treat \\(s_j - s_{y_i} + 1\\) as a whole and the entire expression will make a lot more sense.</p> <p>Furthermore:</p> \\[ \\begin{align} \\dfrac{\\partial s_k}{\\partial W} &amp;= \\begin{bmatrix}                                     \\dfrac{\\partial s_k}{\\partial W_{11}} &amp; \\dfrac{\\partial s_k}{\\partial W_{12}} &amp; \\dots  &amp; \\dfrac{\\partial s_k}{\\partial W_{1C}} \\\\                                     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\                                     \\dfrac{\\partial s_k}{\\partial W_{D1}} &amp; \\dfrac{\\partial s_k}{\\partial W_{D2}} &amp; \\dots  &amp; \\dfrac{\\partial s_k}{\\partial W_{DC}}                                     \\end{bmatrix} \\end{align} \\] <p>At each matrix slot, the value of \\(\\dfrac{\\partial s_k}{\\partial W_{mn}}\\) depends on whether \\(s_k\\) is 0.</p> <p>If it is, the derivative is 0 for sure.</p> <p>If not,</p> \\[ \\begin{align} s_k &amp;= s_k - s_{y_i} + 1 \\\\     &amp;= \\mathbf{X} \\cdot \\mathbf{W_k} - \\mathbf{X} \\cdot \\mathbf{W_{y_i}} + 1 \\end{align} \\] <p>Note, the \\(W_k\\) here means the kth column of the weight matrix.</p> <p>Thus, (suppose \\(s_k\\) is not 0), \\(\\dfrac{\\partial s_k}{\\partial W_{mn}} = X_n\\)</p>"},{"location":"content/SVM/#code","title":"Code","text":"<p>The theory is a mind-boggling. Let's walk through the code along with the theories we've just cracked.</p>"},{"location":"content/SVM/#naive-approach","title":"Naive Approach","text":"<pre><code>def svm_loss_naive(W, X, y, reg):\n    \"\"\"\n    Structured SVM loss function, naive implementation (with loops).\n\n    Inputs have dimension D, there are C classes, and we operate on minibatches\n    of N examples.\n\n    Inputs:\n    - W: A numpy array of shape (D, C) containing weights.\n    - X: A numpy array of shape (N, D) containing a minibatch of data.\n    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n      that X[i] has label c, where 0 &lt;= c &lt; C.\n    - reg: (float) regularization strength\n\n    Returns a tuple of:\n    - loss as single float\n    - gradient with respect to weights W; an array of same shape as W\n    \"\"\"\n    dW = np.zeros(W.shape)  # initialize the gradient as zero\n\n    # compute the loss and the gradient\n    num_classes = W.shape[1]\n    num_train = X.shape[0]\n    loss = 0.0\n\n    for i in range(num_train):\n        scores = X[i].dot(W)\n        correct_class_score = scores[y[i]]\n        for j in range(num_classes):\n            if j == y[i]:\n                continue\n            margin = scores[j] - correct_class_score + 1  # note delta = 1\n            if margin &gt; 0:\n                dW[:, j] += X[i]\n                dW[:, y[i]] -= X[i]\n                loss += margin\n\n    # Right now the loss is a sum over all training examples, but we want it\n    # to be an average instead so we divide by num_train.\n    loss /= num_train\n\n    # Add regularization to the loss.\n    loss += reg * np.sum(W * W)\n\n    #############################################################################\n    # TODO:                                                                     #\n    # Compute the gradient of the loss function and store it dW.                #\n    # Rather than first computing the loss and then computing the derivative,   #\n    # it may be simpler to compute the derivative at the same time that the     #\n    # loss is being computed. As a result, you may need to modify some of the   #\n    # code above to compute the gradient.                                       #\n    #############################################################################\n    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n\n    # Average the gradient\n    dW /= num_train\n    dW += 2 * reg * W\n\n    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n\n    return loss, dW\n</code></pre>"},{"location":"content/SVM/#vectorized-approach","title":"Vectorized Approach","text":"<pre><code>def svm_loss_vectorized(W, X, y, reg):\n    \"\"\"\n    Structured SVM loss function, vectorized implementation.\n\n    Inputs and outputs are the same as svm_loss_naive.\n    \"\"\"\n    loss = 0.0\n    dW = np.zeros(W.shape)  # initialize the gradient as zero\n\n    #############################################################################\n    # TODO:                                                                     #\n    # Implement a vectorized version of the structured SVM loss, storing the    #\n    # result in loss.                                                           #\n    #############################################################################\n    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n\n    scores = np.dot(X, W)\n    scores = np.maximum(0, scores - scores[range(len(scores)), y].reshape(-1, 1) + 1)\n    scores[range(len(scores)), y] = 0\n\n    loss = np.sum(scores) / X.shape[0]\n    loss += reg * np.sum(W * W)\n\n    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n\n    #############################################################################\n    # TODO:                                                                     #\n    # Implement a vectorized version of the gradient for the structured SVM     #\n    # loss, storing the result in dW.                                           #\n    #                                                                           #\n    # Hint: Instead of computing the gradient from scratch, it may be easier    #\n    # to reuse some of the intermediate values that you used to compute the     #\n    # loss.                                                                     #\n    #############################################################################\n    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n\n    dvalues = (scores &gt; 0).astype(float)\n    dvalues[range(len(dvalues)), y] -= np.sum(dvalues, axis=1)\n    dW = np.dot(X.T, dvalues) / X.shape[0]\n\n    dW += 2 * reg * W\n\n    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n\n    return loss, dW\n</code></pre>"}]}